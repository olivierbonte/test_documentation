{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p> <p>Source code of this documentation to be found at: https://github.com/olivierbonte/test_documentation</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"HPC/intro/","title":"Introduction to the HPC","text":"<p>This serves as an example page added to the documentation.</p> <p>For all info on the HPC @ UGhent, the reader is referred to the official HPC website.</p>"},{"location":"HPC/intro/#extra-header","title":"Extra header","text":"<p>I am a different user that wanted to contribute some text here. </p>"},{"location":"HPC/tier1_guide/","title":"User's guide to get started with Tier-1","text":"<p>The VSC Tier-1 computing iTo log inproject should be able to access the \"hcg_heat\" repository. Obviously,you must use your UGent credentials, usinGlobus nThe Storage4Climate project has a storage capacity of ~5PB in Tier-1 Data (\"storage4climate\" repository). H-CEL can only access a small part of that storage but still the resources are comparable to what we currently have in Tier-2 (400TB by the end of 2024). Additionally, H-CEL has 200TB available for the HEAT project in the \"hcg_heat\" repository.y allows you to transfer, but also to create, rename, delete or download directories (toolbar in the central part of the image above). However, it should be noted that the \"storage4climate\" folders, even the ones we created, belong to another user (HPC staff). This means that we will not be able to delete them. From ManGO we will not even get the option to delete them and from Globus we can launch a job to do it, but we will get an error.ink \"Partner organizations: VSC account\". Then you must click on \"Enter Portal\" to access the iRODS vsc-climate zone.</p> <p>Once inside you should be able to see the different repositories you are a member of. Everyone should be able to see a repository called \"public\" and another called \"storage4climate\". Additionally, members of the HEAT project should be able to access the \"hcg_heat\" repository. Obviously, the latter is intended exclusively for HEAT project data, so all data associated with other projects should be stored in the \"storage4climate\" repository. If you do not have access, ask to be added.re consists of three components: Tier-1 Compute, Tier-1 Data and Tier-1 Cloud. Here we focus on describing the first two, which are the components to which we have access.</p>"},{"location":"HPC/tier1_guide/#tier-1-compute","title":"Tier-1 Compute","text":""},{"location":"HPC/tier1_guide/#tier-1-compute_1","title":"Tier-1 Compute","text":"<p>As its name suggests, it is designed for computation, which is performed in the Hortense cluster, hosted by Ghent University, and maintained and supported by the HPC-UGent team. Hortense has an internal name called dodrio.</p> <p>Connecting to Hortense is analogous to connecting to any of the Tier-2 clusters:</p> <pre><code>ssh -X vscXXXXX@tier1.hpc.ugent.be\n</code></pre> <p>Where vscXXXXX is your username.</p> <p>The use of Tier-1 is necessarily associated with a project, in our case the Storage4Climate project. All projects can be consulted using the environment variable <code>$VSC_SCRATCH_PROJECTS_BASE</code>. The directory associated with our project is 2022_200, which is where we should work. To this end, enter the folder:</p> <pre><code>cd /dodrio/scratch/projects/2022_200/project_output/hcel\n</code></pre> <p>And create a working directory with your username:</p> <pre><code>mkdir vscXXXXX\n</code></pre> <p>All your computations should be performed from that directory. The reason for this is that the storage space on <code>$VSC_SCRATCH</code> (personal scratch directory) is very limited. Therefore, you have to be located inside the 2022_200 folder to access the project storage space. The Storage4Climate initiative has approximately 40% of the storage available at Hortense (~2PB). However, Tier-1 Compute is not intended for long-term data storage, which means that data should be moved to Tier-1 Data as soon as possible.</p> <p>It is useful to link that working directory to your home directory, which in Tier-1 Hortense corresponds to your personal scratch directory (<code>$VSC_SCRATCH</code>):</p> <pre><code>cd $VSC_SCRATCH\nln -s /dodrio/scratch/projects/2022_200/project_output/hcel/vscXXXXX scratch_s4c\n</code></pre> <p>So every time you enter Tier-1 Hortense you can access it by simply executing:</p> <pre><code>cd scratch_s4c\n</code></pre> <p>On the other hand, when sending jobs to queues in Hortense, we only need to include one more line in the header:</p> <pre><code>#PBS -A 2022_204\n</code></pre> <p>Which indicates our subproject (2022_204), or in other words, the code associated with H-CEL. Before submitting jobs to the queue, we need to go to the cluster partition where we have reserved the compute (cpu_milan).</p> <pre><code>module swap cluster/dodrio/cpu_milan\n</code></pre> <p>The total computing capacity for the Storage4Climate project is 54252272 CPU hours in 2024, of which only 130000 are available to H-CEL members. Still, this is a high computational capacity and could be increased in 2025.</p> <p>Finally, note that the modules installed in Tier-1 Hortense may not be the same as those installed in Tier-2 clusters, which could cause incompatibilities with your applications.</p>"},{"location":"HPC/tier1_guide/#tier-1-data","title":"Tier-1 Data","text":"<p>Tier-1 Data is intended for massive data storage. The service is based on the open source software iRODS. This means that we cannot access it through an SSH client, such as Tier-1 Computation or Tier-2, but have to use other clients. There are different options of which here we describe the ManGO and Globus clients, which should be sufficient to manage and transfer data in Tier-1 Data.</p>"},{"location":"HPC/tier1_guide/#mango-portal","title":"ManGO Portal","text":"<p>The ManGO portal is a graphical web interface for Tier-1 Data. It allows users to manage their data in an intuitive way, without any installations.</p> <p>To log in you must use your UGent credentials, using the link \"Partner organizations: VSC account\". Then you must click on \"Enter Portal\" to access the iRODS vsc-climate zone.</p> <p>Once inside you should be able to see the different repositories you are a member of. Everyone should be able to see a repository called \u201cpublic\u201d and another called \u201cstorage4climate\u201d. Additionally, members of the HEAT project should be able to access the \u201chcg_heat\u201d repository. Obviously, the latter is intended exclusively for HEAT project data, so all data associated with other projects should be stored in the \u201cstorage4climate\u201d repository. If you do not have access, ask to be added.</p> <p>When you access one of these repositories for the first time, you may need to create a new collection to store your data by clicking on \u201cAdd collection\u201d (see number 1 in the screenshot below). Collections are similar to a regular directory, but with the ability to add metadata, i.e., information describing the contents of the collections, and a simple permissions management (2). This makes it easier to share data with other users. If you do not add permissions no other person will be able to see your content in the repository. It would be advisable to at least add read rights with the \u201cRecursive\u201d option. (3). The name of the new collection should be recognizable. In \u201cstorage4climate\u201d you should organize the data in the \u201cexternal\u201d and \u201cproject_input\u201d folders, but there is no clear rule on how to do it so simply create a collection when and where you think it is most suitable for you to store your data. In the case of \"hcg_heat\", you could simply create a collection with your name or your username. You can later modify the name of your new collections if you wish by clicking on the pencil next to the name (4).</p> <p></p> <p></p> <p>Finally, if you are the manager of one of the repositories you can add new users through your profile (username in the upper right corner), clicking on the desired repository and then on \"Project details\" and \"Add member\". You can find more detailed information in this link.</p>"},{"location":"HPC/tier1_guide/#globus-data-sharing-platform","title":"Globus data sharing platform","text":"<p>Globus enables high-performance data transfer between systems within the same organization or between organizations. It is a fast, secure, and reliable way to move MB's, TB's and even PB's of data.</p> <p>To log in you must search for \"UGent\" in the organizations search engine, click \"Continue\" and then use your credentials. Once inside you will be able to split the screen into 2 panels (see 1 in the screenshot below) and select two \"Collections\" (2) between which you want to share data. For example, to share data between your Tier-2 and Tier-1 Data accounts you should select the \"VSC UGent Tier-2 filesystems\" collection on one panel and the \"VSC iRODS vsc-climate.irods.hpc.kuleuven.be\" collection on the other (see this link for all available collections). Once selected you can browse through your folders by simply clicking, as usual. To transfer files or folders between the two systems, check the box to the left of the file and folder names (3) and click \"Start\" (4). Note that the file or folder will be moved to the directory where you are currently located on the destination server. You can find all the information in this link.</p> <p></p> <p>Globus not only allows you to transfer, but also to create, rename, delete or download directories (toolbar in the central part of the image above). However, it should be noted that the \"storage4climate\" folders, even the ones we created, belong to another user (HPC staff). This means that we will not be able to delete them. From ManGO we will not even get the option to delete them and from Globus we can launch a job to do it, but we will get an error.</p> <p>The Storage4Climate project has a storage capacity of ~5PB in Tier-1 Data (\"storage4climate\" repository). H-CEL can only access a small part of that storage but still the resources are comparable to what we currently have in Tier-2 (400TB by the end of 2024). Additionally, H-CEL has 200TB available for the HEAT project in the \"hcg_heat\" repository.</p> <p></p>"}]}